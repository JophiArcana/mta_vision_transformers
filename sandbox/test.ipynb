{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1149496617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/workspace/mta_vision_transformers/\")\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable, Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDict\n",
    "from torch.utils._pytree import tree_flatten\n",
    "\n",
    "from core.monitor import Monitor\n",
    "from dataset.construct import ImageDataset\n",
    "from dataset.library import DATASETS\n",
    "from infrastructure import utils\n",
    "from infrastructure.settings import DEVICE, OUTPUT_DEVICE, DTYPE\n",
    "\n",
    "\n",
    "dataset_name, n_classes = DATASETS[\"Common\"][1]\n",
    "    \n",
    "# Ocean: 901085904\n",
    "# Rose: 100390212\n",
    "torch.set_printoptions(linewidth=400, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ncut_pytorch import NCUT\n",
    "\n",
    "from modeling.image_features import ImageFeatures\n",
    "from modeling.openclip_vit import OpenCLIPViT\n",
    "from modeling.vit_extraction import OpenCLIPExtractionViT\n",
    "from visualize.base import construct_per_layer_output_dict\n",
    "\n",
    "\n",
    "def ncut_pre_hook(t: torch.Tensor) -> torch.Tensor:\n",
    "    num_eig = 100\n",
    "    ncut = NCUT(num_eig=num_eig, distance=\"rbf\", indirect_connection=False, device=DEVICE)\n",
    "    return ncut.fit_transform(t.flatten(0, -2))[0].unflatten(0, t.shape[:-1])\n",
    "\n",
    "def norm_post_hook(t: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.norm(t[..., :2], p=2, dim=-1)\n",
    "\n",
    "conditions = {\n",
    "    # 10: {\n",
    "    #     \"condition\": lambda t: t > 0.0118,\n",
    "    #     \"pre_hook\": ncut_pre_hook,\n",
    "    #     \"post_hook\": norm_post_hook,\n",
    "    # },\n",
    "    12: {\n",
    "        \"name\": \"MA\",\n",
    "        \"condition\": lambda t: t > 25.0,\n",
    "        # \"condition\": lambda t: t < -25.0\n",
    "    },\n",
    "    # 16: {\n",
    "    #     \"condition\": lambda t: t > 7.5\n",
    "    # },\n",
    "    15: {\n",
    "        \"name\": \"Artifact\",\n",
    "        \"pre_hook\": ncut_pre_hook,\n",
    "        \"condition\": lambda t: t > 0.015,\n",
    "        # \"condition\": lambda t: t < -0.015,\n",
    "    },\n",
    "    # 16: {\n",
    "    #     \"condition\": lambda t: t < -7.5,\n",
    "    # },\n",
    "}\n",
    "mode = \"mean_concatenation\"\n",
    "model = OpenCLIPExtractionViT(mode, conditions)\n",
    "original_model = OpenCLIPViT()\n",
    "\n",
    "def residual_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    return input_ + tree_flatten(output_)[0][0]\n",
    "    \n",
    "def input_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    return tree_flatten(input_)[0][0]\n",
    "\n",
    "def weight_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    return model_.weight.mT\n",
    "\n",
    "def get_weight_by_name(name: str) -> Callable[[nn.Module, Any, Any], Any]:\n",
    "    def hook(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "        return utils.rgetattr(model_, name).data\n",
    "    return hook\n",
    "\n",
    "def attention_proj_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    return tree_flatten(input_)[0][0] @ model_.in_proj_weight.mT + model_.in_proj_bias\n",
    "\n",
    "def attention_matrix_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    return einops.rearrange(\n",
    "        model_.forward(*input_, attn_mask=None, need_weights=True, average_attn_weights=False)[1][..., :ImageFeatures.N + 1, :ImageFeatures.N + 1],\n",
    "        \"n ... h w -> n h w ...\"\n",
    "    ).to(OUTPUT_DEVICE)\n",
    "    # return einops.rearrange(\n",
    "    #     tree_flatten(output_)[0][0],\n",
    "    #     \"b h n1 n2 -> b n1 n2 h\"\n",
    "    # ).to(OUTPUT_DEVICE)\n",
    "\n",
    "def query_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    QKVw = model_.in_proj_weight\n",
    "    QKVb = model_.in_proj_bias\n",
    "    \n",
    "    D = 1024\n",
    "    Qw, Qb = QKVw[:D], QKVb[:D]\n",
    "    Q = utils.linear_from_wb(Qw, Qb)\n",
    "    return Q.forward(tree_flatten(input_)[0][0]).to(OUTPUT_DEVICE)\n",
    "\n",
    "def key_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    QKVw = model_.in_proj_weight\n",
    "    QKVb = model_.in_proj_bias\n",
    "    \n",
    "    D = 1024\n",
    "    Kw, Kb = QKVw[D:2 * D], QKVb[D:2 * D]\n",
    "    K = utils.linear_from_wb(Kw, Kb)\n",
    "    return K.forward(tree_flatten(input_)[0][0]).to(OUTPUT_DEVICE)\n",
    "\n",
    "def value_hook_fn(model_: nn.Module, input_: Any, output_: Any) -> Any:\n",
    "    QKVw = model_.in_proj_weight\n",
    "    QKVb = model_.in_proj_bias\n",
    "    \n",
    "    D = 1024\n",
    "    Vw, Vb = QKVw[2 * D:], QKVb[2 * D:]\n",
    "    V = utils.linear_from_wb(Vw, Vb)\n",
    "    return V.forward(tree_flatten(input_)[0][0]).to(OUTPUT_DEVICE)\n",
    "\n",
    "\n",
    "monitor_config = OrderedDict({\n",
    "    \"model.visual.transformer.resblocks\": OrderedDict({\n",
    "        \"\": [\n",
    "            # (\"layer_input\", input_hook_fn),\n",
    "            (\"layer_output\", Monitor.default_hook_fn),\n",
    "        ],\n",
    "        # \"ln_1\": \"layer_norm1_output\",  # \"norm1\"\n",
    "        # \"attn\": [\n",
    "        #     # (\"attention_input\", input_hook_fn),\n",
    "        #     # (\"query\", query_hook_fn),\n",
    "        #     # (\"key\", key_hook_fn),\n",
    "        #     (\"value\", value_hook_fn),\n",
    "        #     # (\"attention_proj\", attention_proj_hook_fn),\n",
    "        #     # (\"attention_output\", Monitor.default_hook_fn),\n",
    "        #     # (\"attention_matrix\", attention_matrix_hook_fn),\n",
    "        # ],\n",
    "        # \"ln_2\": [\n",
    "        #     (\"intermediate_output\", input_hook_fn),\n",
    "        #     (\"layer_norm2_output\", Monitor.default_hook_fn),  # \"norm2\"\n",
    "        # ],\n",
    "        # \"mlp\": {\n",
    "        #     \"\": \"mlp_output\",\n",
    "        #     \"c_fc\": [\n",
    "        #         # (\"mlp_fc1_input\", input_hook_fn),\n",
    "        #         # (\"mlp_fc1_output_no_bias\", fc_no_bias_hook_fn),\n",
    "        #         (\"mlp_fc1_output\", Monitor.default_hook_fn),\n",
    "        #         # (\"mlp_fc1_weight\", weight_hook_fn),\n",
    "        #     ],\n",
    "        #     \"gelu\": [\n",
    "        #         (\"mlp_activation_output\", Monitor.default_hook_fn),\n",
    "        #     ],\n",
    "        #     # \"c_proj\": \"mlp_fc2\",\n",
    "        # }\n",
    "    })\n",
    "})\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "monitor = Monitor(model, monitor_config)\n",
    "model_weights = Monitor(model, OrderedDict({\n",
    "    \"model.visual.transformer.resblocks.attn\": OrderedDict({\n",
    "        # \"\": [\n",
    "        #     (\"QKVw\", get_weight_by_name(\"in_proj_weight\")),\n",
    "        #     (\"QKVb\", get_weight_by_name(\"in_proj_bias\")),\n",
    "        #     (\"out_w\", get_weight_by_name(\"out_proj.weight\")),\n",
    "        #     (\"out_b\", get_weight_by_name(\"out_proj.bias\")),\n",
    "        # ],\n",
    "    }),\n",
    "}))\n",
    "\n",
    "# SECTION: Set up dataset\n",
    "batch_size = 50\n",
    "dataset = ImageDataset(dataset_name, split=\"train\", return_original_image=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(DEVICE))\n",
    "original_images, images = next(iter(dataloader))\n",
    "\n",
    "torch.set_default_device(DEVICE)\n",
    "per_metric_output_dict = monitor.reset()\n",
    "model_dict = model_weights.reset()\n",
    "with torch.no_grad():\n",
    "    output = model.forward(images)\n",
    "    # original_output = original_model(images)\n",
    "\n",
    "per_layer_output_dict = construct_per_layer_output_dict(per_metric_output_dict)\n",
    "model_dict = [\n",
    "    dict(zip(model_dict.keys(), next(zip(*v))))\n",
    "    for v in zip(*model_dict.values())\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(linewidth=400, sci_mode=False)\n",
    "\n",
    "# def profile(name: str, relative_error: torch.Tensor) -> None:\n",
    "#     print(f\"{name} --- max: {relative_error.max().item()}, mean: {relative_error.mean().item()}, min: {relative_error.min().item()}\")\n",
    "\n",
    "# re = torch.abs(output[0][0] / original_output[0] - 1)\n",
    "# ae = torch.abs(output[0][0] - original_output[0])\n",
    "\n",
    "# profile(f\"{mode} relative error\", re)\n",
    "# profile(f\"{mode} absolute error\", ae)\n",
    "\n",
    "# print(ae)\n",
    "# print(original_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA: 129/12850\n",
      "AS: 1556/12850\n"
     ]
    }
   ],
   "source": [
    "# SECTION: Visualize original images\n",
    "%matplotlib inline\n",
    "from core.attention_sink import massive_token_heuristic\n",
    "from modeling.image_features import ImageFeatures\n",
    "from visualize.base import visualize_images_with_mta, get_rgb_colors\n",
    "from visualize.attention import compute_attention_contribution\n",
    "\n",
    "torch.set_default_device(OUTPUT_DEVICE)\n",
    "\n",
    "# SECTION: Massive token heuristic\n",
    "if isinstance(output, tuple):\n",
    "    mta_masks: Dict[str, torch.Tensor] = {\n",
    "        k: torch.load(f\"experiments/saved_masks/{k}_mask{batch_size}.pt\", map_location=OUTPUT_DEVICE)\n",
    "        for k in (\"MA\", \"AS\")\n",
    "    }\n",
    "    # mta_masks: Dict[int, torch.Tensor] = {k: v.to(OUTPUT_DEVICE) for k, v in output[1].items()}\n",
    "else:\n",
    "    layer_idx = 15\n",
    "    mta_masks: Dict[str, torch.Tensor] = {\"MA\": massive_token_heuristic(layer_idx, per_metric_output_dict).to(OUTPUT_DEVICE)}\n",
    "\n",
    "for k, v in mta_masks.items():\n",
    "    print(f\"{k}: {v.sum().item()}/{v.numel()}\")\n",
    "\n",
    "# features = ImageFeatures(per_layer_output_dict, mta_masks, mode, DEVICE)\n",
    "features = ImageFeatures(per_layer_output_dict, {}, mode, DEVICE)\n",
    "\n",
    "# # Update ImageFeatures data structure with attention contributions\n",
    "# attention_contributions = [\n",
    "#     compute_attention_contribution(features, layer_idx, model_dict, mta_masks)\n",
    "#     for layer_idx in range(features.num_layers)\n",
    "# ]\n",
    "# for k, alias in mta_aliases.items():\n",
    "#     features.update((\"attention_contribution\", alias), torch.stack([\n",
    "#         attention_contribution[k].to(OUTPUT_DEVICE)\n",
    "#         for attention_contribution in attention_contributions\n",
    "#     ], dim=0))\n",
    "\n",
    "# # Visualize images\n",
    "# for mask in mta_masks.values():\n",
    "#     visualize_images_with_mta(original_images.to(OUTPUT_DEVICE), mask.to(OUTPUT_DEVICE))\n",
    "\n",
    "try:\n",
    "    rgb_assignment\n",
    "except NameError:\n",
    "    rgb_fname = \"sandbox/rgb_assignment.pt\"\n",
    "    if not os.path.exists(rgb_fname):\n",
    "        color_layer_idx = 10    # min(mta_masks.keys())\n",
    "        rgb_assignment = get_rgb_colors(features, color_layer_idx, \"layer_output\", False)\n",
    "        torch.save(rgb_assignment, rgb_fname)\n",
    "    else:\n",
    "        rgb_assignment = torch.load(rgb_fname, map_location=OUTPUT_DEVICE)\n",
    "\n",
    "# # highlight = torch.LongTensor((\n",
    "# #     (1, 5, 4),\n",
    "# #     (4, 15, 8),\n",
    "# # ))\n",
    "# highlight = torch.load(\"./sandbox/artifact_indices.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 11 ========================================================================================================================\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "asdfuiopqwer, 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 103\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# for sk, alias in mta_aliases.items():\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     visualize_feature_norms_per_image(features, layer_idx, (\"attention_contribution\", alias), cmap=\"gray\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     visualize_features_per_image(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# visualize_feature_norms_per_image(features, layer_idx, \"query\", cmap=\"gray\")\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# visualize_feature_norms_per_image(features, layer_idx, \"key\", cmap=\"gray\")\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# \"layer_norm1_output\",\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# \"query\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#     cmap=\"gray\",\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mvisualize_features_per_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmta_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# mask,\u001b[39;49;00m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# highlight,\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# visualize_feature_norms_per_image(features, layer_idx, metric_name, cmap=\"gray\")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# visualize_qk_projection_per_image(output_dict)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# continue\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# # visualize_feature_norms_per_image(\"attention_input\", output_dict[\"attention_input\"], cmap=\"pink\")\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# # visualize_feature_norms_per_image(\"attention_proj\", output_dict[\"attention_proj\"], cmap=\"viridis\")\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/mta_vision_transformers/visualize/base.py:123\u001b[0m, in \u001b[0;36mvisualize_features_per_image\u001b[0;34m(features, layer_idx, metric_name, mta_mask, use_all, highlight)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_features_per_image\u001b[39m(\n\u001b[1;32m    116\u001b[0m     features: ImageFeatures,\n\u001b[1;32m    117\u001b[0m     layer_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     highlight: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     rgb_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_rgb_colors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_all\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [B x H x W x 3]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39mNUM_VISUALIZED_IMAGES, figsize\u001b[38;5;241m=\u001b[39m(PLOT_SCALE \u001b[38;5;241m*\u001b[39m NUM_VISUALIZED_IMAGES, PLOT_SCALE))\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax_idx, image_features \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rgb_features[VISUALIZED_INDICES]):\n",
      "File \u001b[0;32m/workspace/mta_vision_transformers/visualize/base.py:104\u001b[0m, in \u001b[0;36mget_rgb_colors\u001b[0;34m(features, layer_idx, key, use_all)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ncut_features \u001b[38;5;241m=\u001b[39m ncut\u001b[38;5;241m.\u001b[39mfit(fit_features)\u001b[38;5;241m.\u001b[39mtransform(image_features)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# fit_features = features.get(layer_idx=layer_idx, key=key, include=(ImageFeatures.IMAGE,), exclude=(\"MA\",))  # [? x D]\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     ncut_features \u001b[38;5;241m=\u001b[39m \u001b[43mncut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# print(fit_features.shape, image_features.shape)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# ncut_features = ncut.fit(fit_features).transform(image_features)                            # [(bsz h w) x d]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m rgb_colors \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    109\u001b[0m     generate_rgb_from_tsne_3d(ncut_features),\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(bsz h w) c -> bsz h w c\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mImageFeatures\u001b[38;5;241m.\u001b[39mH, w\u001b[38;5;241m=\u001b[39mImageFeatures\u001b[38;5;241m.\u001b[39mW,\n\u001b[1;32m    111\u001b[0m )\u001b[38;5;241m.\u001b[39mto(OUTPUT_DEVICE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nystrom_ncut/sampling_utils.py:181\u001b[0m, in \u001b[0;36mOnlineTransformerSubsampleFit.fit_transform\u001b[0;34m(self, features, precomputed_sampled_indices)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    features (torch.Tensor): input features, shape (n_samples, n_features)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m unsampled_indices, V_unsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_helper(features, precomputed_sampled_indices)\n\u001b[0;32m--> 181\u001b[0m V_sampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsampled_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m*\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], V_sampled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), device\u001b[38;5;241m=\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nystrom_ncut/kernel/kernel_ncut.py:93\u001b[0m, in \u001b[0;36mKernelNCutBaseTransformer.transform\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     88\u001b[0m     kernelized_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\n\u001b[1;32m     89\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcos(W_features),\n\u001b[1;32m     90\u001b[0m         torch\u001b[38;5;241m.\u001b[39msin(W_features),\n\u001b[1;32m     91\u001b[0m     ), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)                                   \u001b[38;5;66;03m# [... x m x (2 * kernel_dim)]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m row_sum \u001b[38;5;241m=\u001b[39m kernelized_features \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]                           \u001b[38;5;66;03m# [... x m x 1]\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(row_sum \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masdfuiopqwer, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39msum(row_sum\u001b[38;5;250m \u001b[39m\u001b[38;5;241m<\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m normalized_kernelized_features \u001b[38;5;241m=\u001b[39m kernelized_features \u001b[38;5;241m/\u001b[39m (row_sum \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)     \u001b[38;5;66;03m# [... x m x (2 * kernel_dim)]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_kernelized_features \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_matrix\n",
      "\u001b[0;31mAssertionError\u001b[0m: asdfuiopqwer, 11"
     ]
    }
   ],
   "source": [
    "# SECTION: Per image visualization code\n",
    "%matplotlib inline\n",
    "from visualize.base import (\n",
    "    visualize_features_per_image,\n",
    "    visualize_feature_norms_per_image,\n",
    ")\n",
    "from visualize.attention import (\n",
    "    compute_attention_contribution,\n",
    "    visualize_attention_matrix_per_image,\n",
    "    visualize_attention_weights_from_ma_per_image,\n",
    "    visualize_incoming_attention_per_image,\n",
    "    visualize_attention_to_MA_per_image,\n",
    "    visualize_attention_from_CLS_per_image,\n",
    "    visualize_attention_weights_per_image,\n",
    ")\n",
    "from visualize.projections import (\n",
    "    visualize_qk_projection_per_image,\n",
    "    visualize_qk_projection_per_image2,\n",
    "    visualize_pc_projection_per_image,\n",
    "    visualize_feature_values_by_pca,\n",
    ")\n",
    "\n",
    "# for layer_idx in [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]:\n",
    "# for layer_idx in [13]:\n",
    "for layer_idx in [11, 12, 13, 14, 15, 16, 17, 18]:\n",
    "    print(f\"Layer {layer_idx} {'=' * 120}\")\n",
    "    output_dict = per_layer_output_dict[layer_idx]\n",
    "    \n",
    "    # min_cutoff_idx = min((float(\"inf\"), *filter(lambda l: l >= layer_idx, mta_masks.keys())))\n",
    "    # mask = mta_masks.get(min_cutoff_idx, None)\n",
    "    mask = mta_masks[\"MA\"]\n",
    "    \n",
    "    # for sk, alias in mta_aliases.items():\n",
    "    #     visualize_feature_norms_per_image(features, layer_idx, (\"attention_contribution\", alias), cmap=\"gray\")\n",
    "    #     visualize_features_per_image(\n",
    "    #         features, layer_idx, (\"attention_contribution\", alias),\n",
    "    #         mask, use_all=False, highlight=None,\n",
    "    #     )\n",
    "    \n",
    "    # def rank_k(k: int) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    #     def fn(A: torch.Tensor) -> torch.Tensor:\n",
    "    #         utils.reset_seed()\n",
    "    #         # U, S, V = torch.svd_lowrank(torch.log(A), q=100)\n",
    "    #         # return torch.softmax((U[:, k:k + 1] * S[k:k + 1]) @ V[:, k:k + 1].mT, dim=1)\n",
    "    #         U, S, V = torch.svd_lowrank(A, q=100)\n",
    "    #         return (U[:, k:k + 1] * S[k:k + 1]) @ V[:, k:k + 1].mT\n",
    "    #     fn.__name__ = f\"rank{k}\"\n",
    "    #     return fn\n",
    "    \n",
    "    # attention_matrix: torch.Tensor = output_dict[\"attention_matrix\"]\n",
    "    # visualize_attention_matrix_per_image(\n",
    "    #     layer_idx, attention_matrix, mta_masks, transform_func=None, per_head=True,\n",
    "    #     rescale_func=lambda t: torch.log2(t + 1), subsample=1.0, cmap=\"viridis\", cmap_scale=\"arcsinh\",\n",
    "    # )\n",
    "    # visualize_attention_to_MA_per_image(layer_idx, attention_matrix, mta_masks[\"MA\"], exclude_self=False, per_head=True)\n",
    "    # visualize_attention_from_CLS_per_image(layer_idx, attention_matrix, mta_masks[\"MA\"] + mta_masks[\"AS\"], exclude_MA=True, per_head=False)\n",
    "    # for k in range(3):\n",
    "    #     visualize_attention_matrix_per_image(\n",
    "    #         features, layer_idx, mta_aliases, transform_func=rank_k(k), per_head=False,\n",
    "    #         rescale_func=lambda t: torch.log2(t + 1), cmap_scale=\"linear\", subsample=1.0, cmap=\"viridis\",\n",
    "    #     )\n",
    "    # visualize_attention_matrix_per_image(\n",
    "    #     features, layer_idx, mta_aliases, rank_approximation=3, per_head=False,\n",
    "    #     rescale_func=lambda t: torch.log2(t + 1), cmap=\"viridis\",\n",
    "    # )\n",
    "    # visualize_attention_weights_from_ma_per_image(features, layer_idx, mta_masks, mta_aliases, cmap=\"viridis\")\n",
    "    # visualize_incoming_attention_per_image(features, layer_idx, cmap=\"gray\")\n",
    "    \n",
    "    # visualize_attention_weights_per_image(\n",
    "    #     features, layer_idx, mta_masks, mta_aliases, (\"linear\", 2), rgb_assignment,\n",
    "    #     per_head=False,\n",
    "    # )\n",
    "    \n",
    "    # visualize_feature_norms_per_image(features, layer_idx, \"mlp_output\", cmap=\"gray\")\n",
    "    # visualize_pc_projection_per_image(features, layer_idx, \"attention_input\", modes=[\n",
    "    #     (\"linear\", 0),\n",
    "    #     (\"ncut\", 0),\n",
    "    #     (\"ncut\", 1),\n",
    "    # ])\n",
    "    # visualize_feature_norms_per_image(features, layer_idx, \"query\", cmap=\"gray\")\n",
    "    # visualize_feature_norms_per_image(features, layer_idx, \"key\", cmap=\"gray\")\n",
    "    \n",
    "    for metric_name in (\n",
    "        # \"layer_norm1_output\",\n",
    "        # \"query\",\n",
    "        # \"key\",\n",
    "        # \"value\",\n",
    "        # \"attention_output\",\n",
    "        # \"intermediate_output\",\n",
    "        # \"layer_norm2_output\",\n",
    "        # \"mlp_fc1_output\",\n",
    "        # \"mlp_activation_output\",\n",
    "        # \"mlp_output\",\n",
    "        \"layer_output\",\n",
    "    ):\n",
    "        # continue\n",
    "        # visualize_feature_norms_per_image(\n",
    "        #     features,\n",
    "        #     layer_idx,\n",
    "        #     metric_name,\n",
    "        #     cmap=\"gray\",\n",
    "        # )\n",
    "        visualize_features_per_image(\n",
    "            features,\n",
    "            layer_idx,\n",
    "            metric_name,\n",
    "            mta_mask=None,  # mask,\n",
    "            use_all=False,\n",
    "            highlight=None, # highlight,\n",
    "        )\n",
    "        # visualize_feature_norms_per_image(features, layer_idx, metric_name, cmap=\"gray\")\n",
    "        # visualize_qk_projection_per_image(output_dict)\n",
    "        # continue\n",
    "        # visualize_pc_projection_per_image(features, layer_idx, metric_name, modes=[\n",
    "        #     (\"linear\", 0),\n",
    "        #     (\"linear\", 1),\n",
    "        #     # (\"ncut_pca\", 0),\n",
    "        #     # (\"ncut_pca\", 1),\n",
    "        #     # (\"ncut_pca\", 2),\n",
    "        #     # (\"ncut_pca\", 3),\n",
    "        #     # (\"recursive_ncut\", 0),\n",
    "        #     # (\"recursive_ncut\", 1),\n",
    "        #     # (\"axis_align_norm\", 0),\n",
    "        #     # (\"axis_align_norm\", 1),\n",
    "        #     # (\"axis_align_norm\", 2),\n",
    "        #     # (\"axis_align_norm\", 3),\n",
    "        #     # (\"axis_align_norm\", 4),\n",
    "        #     # (\"axis_align_norm\", 5),\n",
    "        #     # (\"ncut\", 0),\n",
    "        #     # (\"ncut\", 1),\n",
    "        #     # (\"ncut\", 2),\n",
    "        #     # (\"ncut\", 3),\n",
    "        # ])\n",
    "     \n",
    "        \n",
    "    # # visualize_feature_norms_per_image(\"attention_input\", output_dict[\"attention_input\"], cmap=\"pink\")\n",
    "    # # visualize_feature_norms_per_image(\"attention_proj\", output_dict[\"attention_proj\"], cmap=\"viridis\")\n",
    "    \n",
    "    continue\n",
    "    visualize_qk_projection_per_image(\n",
    "        features, layer_idx, model_dict, \n",
    "        p=0.0, aggregate_func=torch.mean,\n",
    "    )\n",
    "    \n",
    "    # # visualize_projections_per_image(output_dict, aggregate_func=lambda t: torch.median(t, dim=-2).values, aggregate_name=\"median\")\n",
    "    # # visualize_projections_per_image(output_dict, aggregate_func=lambda t: torch.max(t, dim=-2).values, aggregate_name=\"max\")\n",
    "    # visualize_feature_norms_per_image(\"attention_output\", output_dict[\"attention_output\"], cmap=\"bone\")\n",
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Comparison of linear projections between layers\n",
    "%matplotlib inline\n",
    "from visualize.projections import compare_pc_projection_across_layers\n",
    "\n",
    "compare_pc_projection_across_layers(features, 10, 11, \"layer_output\", rgb_assignment, mode=(\"linear\", 0), highlight=torch.argwhere(mta_masks[12]))\n",
    "raise Exception()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SECTION: Per layer visualization code\n",
    "# %matplotlib inline\n",
    "# from core.visualize import visualize_feature_norms_per_layer\n",
    "\n",
    "# include = [\"layer_output\"]\n",
    "# for metric in include:\n",
    "#     stacked_metric_output = stacked_layer_output_dict[metric]\n",
    "#     visualize_feature_norms_per_layer(metric, stacked_metric_output, mta_mask, mta_indices, rgb_assignment, fns={\n",
    "#         \"norm\": lambda t: torch.norm(t, p=2, dim=-1),\n",
    "#         \"inf_norm\": lambda t: torch.norm(t, p=torch.inf, dim=-1),\n",
    "#         \"max\": lambda t: torch.max(t, dim=-1).values,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Histogram value visualization\n",
    "%matplotlib inline\n",
    "from visualize.projections import visualize_feature_values_by_pca\n",
    "\n",
    "include = {\n",
    "    \"mlp_fc1_input\": {\"align_layers\": True},\n",
    "    \"mlp_fc1_output\": {\"fn\": lambda t: torch.mean(t, dim=0), \"align_layers\": False},\n",
    "    # \"mlp_fc1_output_no_bias\": {\"fn\": lambda t: torch.mean(t, dim=0), \"align_layers\": False},\n",
    "}\n",
    "# include = {\"layer_norm2_input\", \"layer_norm2_output\"}\n",
    "# for metric, metric_kwargs in include.items():\n",
    "#     visualize_feature_values_by_channel(metric, stacked_layer_output_dict[metric], **metric_kwargs)\n",
    "\n",
    "\n",
    "# cutoff_layer = min(mta_masks.keys())\n",
    "for layer_idx in range(13, 24):\n",
    "# for layer_idx in [7]:\n",
    "    print(f\"Layer {layer_idx} {'=' * 120}\")\n",
    "    # min_cutoff_idx = min((float(\"inf\"), *filter(lambda l: l >= layer_idx, mta_masks.keys())))\n",
    "    # # min_cutoff_idx = min((float(\"inf\"), *filter(lambda l: l > min_cutoff_idx, mta_masks.keys())))\n",
    "    # mask = mta_masks.get(min_cutoff_idx, None)\n",
    "    for metric_name in (\n",
    "        # \"attention_input\",\n",
    "        # \"query\",\n",
    "        # \"key\",\n",
    "        \"layer_norm1_output\",\n",
    "        # \"attention_output\",\n",
    "        # \"intermediate_output\",\n",
    "        # \"layer_norm2_output\",\n",
    "        # \"mlp_fc1_output\",\n",
    "        # \"mlp_activation_output\",\n",
    "        # \"mlp_output\",\n",
    "        # \"layer_output\",\n",
    "    ):\n",
    "        visualize_feature_values_by_pca(\n",
    "            features,\n",
    "            layer_idx,\n",
    "            metric_name,\n",
    "            {\"linear\"},   # {\"linear\", \"ncut\", \"recursive_ncut\"},\n",
    "            None,   # mta_masks,\n",
    "            rgb_assignment,\n",
    "            ndim=2,\n",
    "            with_cls=True,\n",
    "            highlight=None,\n",
    "            alpha=1.0,\n",
    "        )\n",
    "    # visualize_feature_norms_by_channel(metric, stacked_layer_output_dict[metric], mta_mask)\n",
    "\n",
    "# visualize_fc_weights(stacked_layer_output_dict[\"mlp_fc1_input\"], stacked_layer_output_dict[\"mlp_fc1_weight\"], mta_mask, mean=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
